{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Cell 1: Setup and Imports\n",
    "# ====================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cell 2: Create Directories\n",
    "# ====================================\n",
    "import os\n",
    "\n",
    "directories = ['data', 'models', 'results']\n",
    "for dir_name in directories:\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "    print(f\"✓ Directory '{dir_name}' ready\")\n"
   ],
   "id": "825de564e32830ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cell 3: Data Collection\n",
    "# ====================================\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 1: DATA COLLECTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create sample dataset\n",
    "def create_sample_data(n_samples=3000):\n",
    "    \"\"\"Create sample Amazon-like reviews\"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    # Positive samples (rating 4-5)\n",
    "    positive_texts = [\n",
    "        \"This product is absolutely amazing! Highly recommended.\",\n",
    "        \"Excellent quality, exceeded my expectations. Worth every penny!\",\n",
    "        \"Best purchase ever! Love it! Will definitely buy again.\",\n",
    "        \"Outstanding product, will recommend to friends. Very satisfied.\",\n",
    "        \"Perfect! Exactly what I needed. Great quality and fast shipping.\",\n",
    "        \"Superb product! Excellent value for money. Very happy with purchase.\",\n",
    "        \"Fantastic! Works perfectly. Better than expected quality.\",\n",
    "        \"Love this product! Great features and easy to use.\",\n",
    "        \"Brilliant purchase! Exceptional quality and service.\",\n",
    "        \"Amazing product! Exceeded all my expectations. Five stars!\"\n",
    "    ]\n",
    "\n",
    "    # Negative samples (rating 1-2)\n",
    "    negative_texts = [\n",
    "        \"Terrible quality, complete waste of money. Very disappointed.\",\n",
    "        \"Very disappointed, does not work as advertised. Don't buy!\",\n",
    "        \"Poor quality, broke after one use. Would not recommend.\",\n",
    "        \"Not worth the price, very bad quality. Returning immediately.\",\n",
    "        \"Horrible experience, waste of time and money. Avoid this product!\",\n",
    "        \"Cheaply made, fell apart quickly. Terrible customer service.\",\n",
    "        \"Defective product, doesn't work at all. Very frustrating.\",\n",
    "        \"Absolutely awful! Not as described. Complete disappointment.\",\n",
    "        \"Useless product, total waste. Save your money!\",\n",
    "        \"Worst purchase ever! Broke immediately. Very poor quality.\"\n",
    "    ]\n",
    "\n",
    "    # Neutral samples (rating 3)\n",
    "    neutral_texts = [\n",
    "        \"It's okay, nothing special. Average product for the price.\",\n",
    "        \"Average product, does the job but nothing extraordinary.\",\n",
    "        \"Not bad, but not great either. Acceptable quality.\",\n",
    "        \"Decent for the price. Works as expected, nothing more.\",\n",
    "        \"Works as expected, nothing special. Fair quality.\",\n",
    "        \"Acceptable product, meets basic expectations.\",\n",
    "        \"It's fine, does what it says. Neither good nor bad.\",\n",
    "        \"Mediocre quality, but functional. Could be better.\",\n",
    "        \"Standard product, nothing to complain about.\",\n",
    "        \"Fair quality, serves its purpose adequately.\"\n",
    "    ]\n",
    "\n",
    "    # Generate dataset\n",
    "    texts, ratings = [], []\n",
    "\n",
    "    n_positive = int(n_samples * 0.5)\n",
    "    n_negative = int(n_samples * 0.3)\n",
    "    n_neutral = n_samples - n_positive - n_negative\n",
    "\n",
    "    # Add variety by repeating and slight modifications\n",
    "    texts.extend(positive_texts * (n_positive // len(positive_texts) + 1))\n",
    "    ratings.extend([5] * n_positive)\n",
    "\n",
    "    texts.extend(negative_texts * (n_negative // len(negative_texts) + 1))\n",
    "    ratings.extend([1] * n_negative)\n",
    "\n",
    "    texts.extend(neutral_texts * (n_neutral // len(neutral_texts) + 1))\n",
    "    ratings.extend([3] * n_neutral)\n",
    "\n",
    "    # Trim to exact size\n",
    "    texts = texts[:n_samples]\n",
    "    ratings = ratings[:n_samples]\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'text': texts,\n",
    "        'rating': ratings\n",
    "    })\n",
    "\n",
    "    # Shuffle\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Create and save data\n",
    "df = create_sample_data(3000)\n",
    "\n",
    "# Add sentiment labels\n",
    "def rating_to_sentiment(rating):\n",
    "    if rating <= 2:\n",
    "        return 0, 'negative'\n",
    "    elif rating == 3:\n",
    "        return 1, 'neutral'\n",
    "    else:\n",
    "        return 2, 'positive'\n",
    "\n",
    "df['sentiment'], df['sentiment_label'] = zip(*df['rating'].apply(rating_to_sentiment))\n",
    "\n",
    "# Save\n",
    "df.to_csv('data/raw_reviews.csv', index=False)\n",
    "\n",
    "print(f\"✓ Dataset created: {len(df)} samples\")\n",
    "print(f\"\\nClass Distribution:\")\n",
    "print(df['sentiment_label'].value_counts())\n",
    "print(f\"\\n{df.head()}\")\n"
   ],
   "id": "2ce88e9bf2dd969f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cell 4: Data Analysis\n",
    "# ====================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Statistics\n",
    "df['text_length'] = df['text'].apply(len)\n",
    "df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "print(\"\\nText Length Statistics:\")\n",
    "print(df['text_length'].describe())\n",
    "\n",
    "print(\"\\nWord Count Statistics:\")\n",
    "print(df['word_count'].describe())\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Class distribution\n",
    "df['sentiment_label'].value_counts().plot(kind='bar', ax=axes[0,0], color='skyblue')\n",
    "axes[0,0].set_title('Class Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0,0].set_xlabel('Sentiment')\n",
    "axes[0,0].set_ylabel('Count')\n",
    "\n",
    "# Text length by sentiment\n",
    "for sentiment in df['sentiment_label'].unique():\n",
    "    data = df[df['sentiment_label']==sentiment]['text_length']\n",
    "    axes[0,1].hist(data, alpha=0.6, label=sentiment, bins=30)\n",
    "axes[0,1].set_title('Text Length Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0,1].set_xlabel('Text Length')\n",
    "axes[0,1].legend()\n",
    "\n",
    "# Word count distribution\n",
    "df['word_count'].hist(bins=30, ax=axes[1,0], color='coral')\n",
    "axes[1,0].set_title('Word Count Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1,0].set_xlabel('Word Count')\n",
    "\n",
    "# Rating distribution\n",
    "df['rating'].value_counts().sort_index().plot(kind='bar', ax=axes[1,1], color='lightgreen')\n",
    "axes[1,1].set_title('Rating Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1,1].set_xlabel('Rating')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/data_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ],
   "id": "47699bab30a74d6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cell 5: Text Preprocessing\n",
    "# ====================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2: TEXT PREPROCESSING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and preprocess text\"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"Preprocessing texts...\")\n",
    "df['clean_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "print(\"✓ Preprocessing completed\")\n",
    "print(f\"\\nExample:\")\n",
    "print(f\"Original: {df['text'].iloc[0]}\")\n",
    "print(f\"Cleaned: {df['clean_text'].iloc[0]}\")\n",
    "\n",
    "\n"
   ],
   "id": "bc5b456f6cdfeab0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cell 6: Feature Extraction\n",
    "# ====================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 3: FEATURE EXTRACTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data\n",
    "X = df['clean_text']\n",
    "y = df['sentiment']\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.1875, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Validation set: {len(X_val)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "\n",
    "# TF-IDF Features\n",
    "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_val_tfidf = tfidf.transform(X_val)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "print(f\"\\n✓ TF-IDF features: {X_train_tfidf.shape[1]} dimensions\")\n",
    "\n",
    "\n"
   ],
   "id": "fae5220d9a2fc018"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cell 7: Train Traditional ML Models\n",
    "# ====================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 4: TRAINING TRADITIONAL ML MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import time\n",
    "\n",
    "results = {}\n",
    "\n",
    "# 1. Logistic Regression\n",
    "print(\"\\n[1/3] Training Logistic Regression...\")\n",
    "start = time.time()\n",
    "lr = LogisticRegression(C=1.0, max_iter=1000, random_state=42)\n",
    "lr.fit(X_train_tfidf, y_train)\n",
    "lr_time = time.time() - start\n",
    "\n",
    "y_train_pred_lr = lr.predict(X_train_tfidf)\n",
    "y_val_pred_lr = lr.predict(X_val_tfidf)\n",
    "\n",
    "results['Logistic Regression'] = {\n",
    "    'train_acc': accuracy_score(y_train, y_train_pred_lr),\n",
    "    'val_acc': accuracy_score(y_val, y_val_pred_lr),\n",
    "    'time': lr_time,\n",
    "    'model': lr,\n",
    "    'predictions': y_val_pred_lr\n",
    "}\n",
    "print(f\"✓ Completed in {lr_time:.2f}s | Val Acc: {results['Logistic Regression']['val_acc']:.4f}\")\n",
    "\n",
    "# 2. Linear SVM\n",
    "print(\"\\n[2/3] Training Linear SVM...\")\n",
    "start = time.time()\n",
    "svm = LinearSVC(C=1.0, max_iter=1000, random_state=42)\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "svm_time = time.time() - start\n",
    "\n",
    "y_train_pred_svm = svm.predict(X_train_tfidf)\n",
    "y_val_pred_svm = svm.predict(X_val_tfidf)\n",
    "\n",
    "results['Linear SVM'] = {\n",
    "    'train_acc': accuracy_score(y_train, y_train_pred_svm),\n",
    "    'val_acc': accuracy_score(y_val, y_val_pred_svm),\n",
    "    'time': svm_time,\n",
    "    'model': svm,\n",
    "    'predictions': y_val_pred_svm\n",
    "}\n",
    "print(f\"✓ Completed in {svm_time:.2f}s | Val Acc: {results['Linear SVM']['val_acc']:.4f}\")\n",
    "\n",
    "# 3. Random Forest\n",
    "print(\"\\n[3/3] Training Random Forest...\")\n",
    "start = time.time()\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=20, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train_tfidf, y_train)\n",
    "rf_time = time.time() - start\n",
    "\n",
    "y_train_pred_rf = rf.predict(X_train_tfidf)\n",
    "y_val_pred_rf = rf.predict(X_val_tfidf)\n",
    "\n",
    "results['Random Forest'] = {\n",
    "    'train_acc': accuracy_score(y_train, y_train_pred_rf),\n",
    "    'val_acc': accuracy_score(y_val, y_val_pred_rf),\n",
    "    'time': rf_time,\n",
    "    'model': rf,\n",
    "    'predictions': y_val_pred_rf\n",
    "}\n",
    "print(f\"✓ Completed in {rf_time:.2f}s | Val Acc: {results['Random Forest']['val_acc']:.4f}\")\n",
    "\n",
    "\n"
   ],
   "id": "81a69ef28662f807"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cell 8: Train Deep Learning Models\n",
    "# ====================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 5: TRAINING DEEP LEARNING MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Prepare sequences\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=100)\n",
    "X_val_seq = pad_sequences(tokenizer.texts_to_sequences(X_val), maxlen=100)\n",
    "X_test_seq = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=100)\n",
    "\n",
    "# 1. MLP Model\n",
    "print(\"\\n[1/2] Training MLP...\")\n",
    "mlp = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(X_train_tfidf.shape[1],)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "mlp.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "start = time.time()\n",
    "history_mlp = mlp.fit(\n",
    "    X_train_tfidf.toarray(), y_train,\n",
    "    validation_data=(X_val_tfidf.toarray(), y_val),\n",
    "    epochs=20, batch_size=32, callbacks=[early_stop], verbose=0\n",
    ")\n",
    "mlp_time = time.time() - start\n",
    "\n",
    "y_val_pred_mlp = np.argmax(mlp.predict(X_val_tfidf.toarray()), axis=1)\n",
    "\n",
    "results['MLP'] = {\n",
    "    'train_acc': history_mlp.history['accuracy'][-1],\n",
    "    'val_acc': history_mlp.history['val_accuracy'][-1],\n",
    "    'time': mlp_time,\n",
    "    'model': mlp,\n",
    "    'history': history_mlp.history,\n",
    "    'predictions': y_val_pred_mlp\n",
    "}\n",
    "print(f\"✓ Completed in {mlp_time:.2f}s | Val Acc: {results['MLP']['val_acc']:.4f}\")\n",
    "\n",
    "# 2. LSTM Model\n",
    "print(\"\\n[2/2] Training LSTM...\")\n",
    "lstm = models.Sequential([\n",
    "    layers.Embedding(10000, 64, input_length=100),\n",
    "    layers.LSTM(32, dropout=0.2),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "lstm.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "start = time.time()\n",
    "history_lstm = lstm.fit(\n",
    "    X_train_seq, y_train,\n",
    "    validation_data=(X_val_seq, y_val),\n",
    "    epochs=10, batch_size=64, callbacks=[early_stop], verbose=0\n",
    ")\n",
    "lstm_time = time.time() - start\n",
    "\n",
    "y_val_pred_lstm = np.argmax(lstm.predict(X_val_seq), axis=1)\n",
    "\n",
    "results['LSTM'] = {\n",
    "    'train_acc': history_lstm.history['accuracy'][-1],\n",
    "    'val_acc': history_lstm.history['val_accuracy'][-1],\n",
    "    'time': lstm_time,\n",
    "    'model': lstm,\n",
    "    'history': history_lstm.history,\n",
    "    'predictions': y_val_pred_lstm\n",
    "}\n",
    "print(f\"✓ Completed in {lstm_time:.2f}s | Val Acc: {results['LSTM']['val_acc']:.4f}\")\n",
    "\n",
    "\n"
   ],
   "id": "c7731ec9c44b333f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cell 9: Model Comparison\n",
    "# ====================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 6: MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create comparison table\n",
    "comparison_data = []\n",
    "for model_name, res in results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'Train Accuracy': f\"{res['train_acc']:.4f}\",\n",
    "        'Val Accuracy': f\"{res['val_acc']:.4f}\",\n",
    "        'Training Time (s)': f\"{res['time']:.2f}\",\n",
    "        'Overfitting': f\"{res['train_acc'] - res['val_acc']:.4f}\"\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('Val Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "\n",
    "# Save\n",
    "comparison_df.to_csv('results/model_comparison.csv', index=False)\n",
    "\n",
    "\n"
   ],
   "id": "124b44f03a108f19"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cell 10: Visualizations\n",
    "# ====================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CREATING VISUALIZATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Confusion matrices\n",
    "for idx, (model_name, res) in enumerate(results.items()):\n",
    "    row, col = idx // 3, idx % 3\n",
    "    cm = confusion_matrix(y_val, res['predictions'])\n",
    "\n",
    "    sns.heatmap(cm, annot=True, fmt='d', ax=axes[row, col], cmap='Blues',\n",
    "                xticklabels=['Neg', 'Neu', 'Pos'],\n",
    "                yticklabels=['Neg', 'Neu', 'Pos'])\n",
    "    axes[row, col].set_title(f\"{model_name}\\nAcc: {res['val_acc']:.3f}\")\n",
    "    axes[row, col].set_xlabel('Predicted')\n",
    "    axes[row, col].set_ylabel('Actual')\n",
    "\n",
    "# Hide extra subplot\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Model comparison bar chart\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "models = [res['Model'] for res in comparison_data]\n",
    "val_accs = [float(res['Val Accuracy']) for res in comparison_data]\n",
    "times = [float(res['Training Time (s)']) for res in comparison_data]\n",
    "\n",
    "axes[0].barh(models, val_accs, color='skyblue')\n",
    "axes[0].set_xlabel('Validation Accuracy')\n",
    "axes[0].set_title('Model Accuracy Comparison')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "axes[1].barh(models, times, color='coral')\n",
    "axes[1].set_xlabel('Training Time (seconds)')\n",
    "axes[1].set_title('Training Time Comparison')\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/model_comparison_chart.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ All visualizations saved to 'results/' folder\")\n",
    "\n"
   ],
   "id": "da43a8cf1fe6a7b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cell 11: Final Report\n",
    "# ====================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL REPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "best_model = comparison_df.iloc[0]\n",
    "\n",
    "print(f\"\\n✓ BEST PERFORMING MODEL\")\n",
    "print(f\"  Model: {best_model['Model']}\")\n",
    "print(f\"  Validation Accuracy: {best_model['Val Accuracy']}\")\n",
    "print(f\"  Training Time: {best_model['Training Time (s)']}s\")\n",
    "print(f\"  Overfitting: {best_model['Overfitting']}\")\n",
    "\n",
    "print(f\"\\n✓ PROJECT COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"\\nGenerated Files:\")\n",
    "print(f\"  - data/raw_reviews.csv\")\n",
    "print(f\"  - results/data_analysis.png\")\n",
    "print(f\"  - results/confusion_matrices.png\")\n",
    "print(f\"  - results/model_comparison.png\")\n",
    "print(f\"  - results/model_comparison.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ],
   "id": "803953a96eaecff7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
